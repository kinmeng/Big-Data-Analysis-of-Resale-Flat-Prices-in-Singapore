shave(upper = TRUE) |>
rplot() + theme_dark() + theme(axis.text.x = element_text(angle = 45, vjust = 0.3, hjust=0.5)) + ggtitle("Correlation plot")
library(sparklyr)
library(dplyr)
library(corrr)
df |> select(floor_area_sqm, remaining_lease_std, resale_price, dist_to_central,
dist_nearest_mall, dist_nearest_mrt) |>
correlate(use = "pairwise.complete.obs", method = "pearson") |>
shave(upper = TRUE) |>
rplot() + theme_dark() + theme(axis.text.x = element_text(angle = 45, vjust = 0.3, hjust=0.5)) + ggtitle("Correlation plot")
library(ggtheme)
library(ggplot2)
df |> select(floor_area_sqm, remaining_lease_std, resale_price, dist_to_central,
dist_nearest_mall, dist_nearest_mrt) |>
correlate(use = "pairwise.complete.obs", method = "pearson") |>
shave(upper = TRUE) |>
rplot() + theme_dark() + theme(axis.text.x = element_text(angle = 45, vjust = 0.3, hjust=0.5)) + ggtitle("Correlation plot")
df |> dbplot_histogram(resale_price)
library(dbplot)
df |> dbplot_histogram(resale_price)
df |> mutate(log_price = log(resale_price)) |> collect() |> dbplot_histogram(log_price)
df |>
collect() |>
select(month, resale_price)|>
mutate(month = as.factor(month)) |>
ggplot(aes(month, resale_price)) +
geom_boxplot() + theme(axis.text.x = element_text(angle = 80, vjust = 1, hjust=1, size =5),
axis.ticks.margin=unit(0,'cm')) + xlab("Date") + ylab('Resale Flat Prices') +
ggtitle("Boxplots of Resale Flat Prices over time")
library(sparklyr)
library(dplyr)
# Connect to a local instance of Spark
sc <- spark_connect(master = "local", version = "3.3.0")
# Define the estimator
scaler <- ft_standard_scaler(
sc,
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
)
scaler
wines_stats <- wines_train |>
summarize(
m = mean(alcohol),
s = sd(alcohol)
) |>
collect()
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
wines_split <- wines |> sdf_random_split(training = 0.7, testing = 0.3, seed = 1337)
wines_train <- wines_split$training
wines_test <- wines_split$testing
wines_train |> sdf_describe(col=c('alcohol', 'volatile_acidity','sulphates'))
wines_train |> dbplot_raster(alcohol, fixed_acidity, fill=n(), resolution=50)
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
pred_1 <- ml_predict(fit_1, dataset = wines_test)
MSE_1 <- ml_regression_evaluator(
pred_1,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
pred_2 <- ml_predict(fit_2, dataset= wines_test)
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
wines_stats <- wines_train |>
summarize(
m = mean(alcohol),
s = sd(alcohol)
) |>
collect()
wines_train <- wines_train |>
mutate(stdz_alcohol = (alcohol - !!wines_stats$m)/ !!wines_stats$s)
spark_write_parquet(wines_train,path='wines/wines_train.parquet')
mean(wines_stats)
sd(wines_stats)
sd(wines_train)
wines_train
mean(wines_train)
summary(wines_train)
summarize(wines_train)
sdf_describe(wines_train)
wines_train <- wines_train |>
mutate(stdz_alcohol = (alcohol - !!wines_stats$m)/ !!wines_stats$s)
sdf_describe(wines_train)
# Copy the rdf data frame to Spark and combine multiple vectors
df <- copy_to(sc, rdf)  |>
ft_vector_assembler(
input_cols = c("v", "w"),
output_col = "features"
)
# Generate two vectors v and w, each with 100000 (pseudo) observations from
# a normal distribution, and use these to create the data frame rdf
rdf <- data.frame(
v = rnorm(100000, mean = 4, sd = 2),
w = rnorm(100000, mean = -4, sd = 0.5)
)
# Copy the rdf data frame to Spark and combine multiple vectors
df <- copy_to(sc, rdf)  |>
ft_vector_assembler(
input_cols = c("v", "w"),
output_col = "features"
)
glimpse(rdf)
# Copy the rdf data frame to Spark and combine multiple vectors
df <- copy_to(sc, rdf)  |>
ft_vector_assembler(
input_cols = c("v", "w"),
output_col = "features"
)
df
glimpse(df)
# Fit the scaling model to the df dataset
scaler_model <- ml_fit(scaler, df)
scaler_model
df |> dbplot_histogram(floor_area_sqm)
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local", version = "3.3.0")
final_dataset <- spark_read_csv(sc, path = "final_dataset.csv")
final_dataset
#write parquet file
spark_write_parquet(final_dataset,'dataset/final.parquet')
#read parquet file as df
df <- spark_read_parquet(sc, 'dataset/final.parquet')
#Ensure that the df has the same number of rows as the original dataset (a sanity check)
sdf_nrow(df)
library(corrr)
library(ggplot2)
df |> select(floor_area_sqm, remaining_lease_std, resale_price, dist_to_central,
dist_nearest_mall, dist_nearest_mrt) |>
correlate(use = "pairwise.complete.obs", method = "pearson") |>
shave(upper = TRUE) |>
rplot() + theme_dark() + theme(axis.text.x = element_text(angle = 45, vjust = 0.3, hjust=0.5)) + ggtitle("Correlation plot")
library(dbplot)
df |> dbplot_histogram(floor_area_sqm)
df |> dbplot_histogram(resale_price)
df |> dbplot_histogram(remaining_lease_std)
df |> mutate(log_price = log(resale_price)) |> collect() |> dbplot_histogram(log_price)
df |>
collect() |>
select(month, resale_price)|>
mutate(month = as.factor(month)) |>
ggplot(aes(month, resale_price)) +
geom_boxplot() + theme(axis.text.x = element_text(angle = 80, vjust = 1, hjust=1, size =5),
axis.ticks.margin=unit(0,'cm')) + xlab("Date") + ylab('Resale Flat Prices') +
ggtitle("Boxplots of Resale Flat Prices over time")
df |>
collect() |>
group_by(storey_range) |>
arrange(storey_range) |>
ggplot(aes(storey_range, resale_price)) +
geom_boxplot() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust=0.9), axis.ticks.margin=unit(0,'cm')) +
xlab("Storey Ranges") +
ylab("Resale Flat Prices") +
ggtitle("Boxplots of Resale Flat Prices over Storey Ranges")
scaler_model |>
ml_transform(df) |>
glimpse()
library(sparklyr)
library(dplyr)
# Connect to a local instance of Spark
sc <- spark_connect(master = "local", version = "3.3.0")
# Define the estimator
scaler <- ft_standard_scaler(
sc,
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
)
scaler
# Generate two vectors v and w, each with 100000 (pseudo) observations from
# a normal distribution, and use these to create the data frame rdf
rdf <- data.frame(
v = rnorm(100000, mean = 4, sd = 2),
w = rnorm(100000, mean = -4, sd = 0.5)
)
glimpse(rdf)
# Copy the rdf data frame to Spark and combine multiple vectors
df <- copy_to(sc, rdf)  |>
ft_vector_assembler(
input_cols = c("v", "w"),
output_col = "features"
)
glimpse(df)
disconnect(sc)
spark_disconnect(sc)
# Connect to a local instance of Spark
sc <- spark_connect(master = "local", version = "3.3.0")
# Define the estimator
scaler <- ft_standard_scaler(
sc,
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
)
# Define the estimator
scaler <- ft_standard_scaler(
sc,
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
)
scaler
# Generate two vectors v and w, each with 100000 (pseudo) observations from
# a normal distribution, and use these to create the data frame rdf
rdf <- data.frame(
v = rnorm(100000, mean = 4, sd = 2),
w = rnorm(100000, mean = -4, sd = 0.5)
)
glimpse(rdf)
# Copy the rdf data frame to Spark and combine multiple vectors
df <- copy_to(sc, rdf)  |>
ft_vector_assembler(
input_cols = c("v", "w"),
output_col = "features"
)
# Copy the rdf data frame to Spark and combine multiple vectors
df <- copy_to(sc, rdf)  |>
ft_vector_assembler(
input_cols = c("v", "w"),
output_col = "features"
)
# Fit the scaling model to the df dataset
scaler_model <- ml_fit(scaler, df)
# Fit the scaling model to the df dataset
scaler_model <- ml_fit(scaler, df)
scaler_model
scaler_model |>
ml_transform(df) |>
glimpse()
pipeline_model <- ml_fit(pipeline, df)
pipeline <- ml_pipeline(scaler)
pipeline_model <- ml_fit(pipeline, df)
pipeline_model
# Read a Parquet file from the LFS into a Spark DataFrame
okc_train <- spark_read_parquet(
sc,
path = "data/okc-train.parquet"
)
# Read a Parquet file from the LFS into a Spark DataFrame
okc_train <- spark_read_parquet(
sc,
path = "data/okc-train.parquet"
)
okc_train <- okc_train |>
select(not_working, age, essay_length)
pipeline <- ml_pipeline(sc) |>
ft_vector_assembler(
input_cols = c("age", "essay_length"),
output_col = "features"
) |>
ft_standard_scaler(
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
) |>
ml_logistic_regression(
features_col = "features_scaled",
label_col = "not_working"
)
pipeline <- ml_pipeline(sc) |>
ft_vector_assembler(
input_cols = c("age", "essay_length"),
output_col = "features"
) |>
ft_standard_scaler(
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
) |>
ml_logistic_regression(
features_col = "features_scaled",
label_col = "not_working"
)
#is_ml_estimator(pipeline) why is it an estimator not a transformer?
#This is because it hasn't been trained - now, you only have instructions for it, but
#it hasn't carried those instructions yet. That's why it's still an estimator
pipeline
okc_train |>
ft_vector_assembler(
input_cols = c("age", "essay_length"),
output_col = "features"
) |>
glimpse()
is_ml_estimator(pipeline)
okc_train |>
ft_vector_assembler(
input_cols = c("age", "essay_length"),
output_col = "features"
) |>
ft_standard_scaler(
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
) |>
glimpse()
pipeline_model <- ml_fit(pipeline, okc_train)
pipeline_model
# distinguish between model parameters and model hyperparameters (lambda is related to
# estimated method, thus, hyperparam - not sure abt this example, but shld be correct )
cv <- ml_cross_validator(
sc,
estimator = pipeline,
estimator_param_maps = list(
logistic_regression = list(
elastic_net_param = c(0.25, 0.75),
reg_param = c(0.001, 0.01)
)
),
evaluator = ml_binary_classification_evaluator(
sc,
label_col = "not_working"
),
num_folds = 10,
parallelism = 1,
seed = 1337
)
cv
cv_model <- ml_fit(cv, okc_train)
ml_validation_metrics(cv_model) |>
arrange(desc(areaUnderROC))
#a hybrid of ridge and lasso
cv_model$metric_name
cv_model$best_model
#read parquet file as df
df <- spark_read_parquet(sc, 'dataset/final.parquet')
#Ensure that the df has the same number of rows as the original dataset (a sanity check)
sdf_nrow(df)
df |> select(floor_area_sqm, remaining_lease_std, resale_price, dist_to_central,
dist_nearest_mall, dist_nearest_mrt) |>
correlate(use = "pairwise.complete.obs", method = "pearson") |>
shave(upper = TRUE) |>
rplot() + theme_dark() + theme(axis.text.x = element_text(angle = 45, vjust = 0.3, hjust=0.5)) + ggtitle("Correlation plot")
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
#spark_disconnect(sc)
wines_split <- wines |> sdf_random_split(training = 0.7, testing = 0.3, seed = 1337)
wines_train <- wines_split$training
wines_test <- wines_split$testing
wines_train |>
sdf_describe(col=c('alcohol','volatile_acidity','sulphates'))
wines_split <- wines |> sdf_random_split(training = 0.7, testing = 0.3, seed = 1337)
wines_train <- wines_split$training
wines_test <- wines_split$testing
wines_train
wines_train |>
sdf_describe(col=c('alcohol','volatile_acidity','sulphates'))
library(dbplot)
wines_train |> dbplot_raster(alcohol, fixed_acidity, fill=n(), resolution=50)
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol) |> tidy()
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates) |> tidy()
fit_2$summary$r2adj
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_2$summary$r2adj
pred_1 <- ml_predict(fit_1, dataset = wines_test)
MSE_1 <- ml_regression_evaluator(
pred_1,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
pred_2 <- ml_predict(fit_2, dataset= wines_test)
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
MSE_1
MSE_2
pred_2 <- ml_predict(fit_2, dataset= wines_test)
pred2
pred_2
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
MSE_1
MSE_2
wines_stats <- wines_train |>
summarize(
m = mean(alcohol),
s = sd(alcohol)
) |>
collect()
wines_train <- wines_train |>
mutate(stdz_alcohol = (alcohol - !!wines_stats$m)/ !!wines_stats$s)
sdf_describe(wines_train, c('stdz_alcohol'))
sdf_describe(wines_train,cols='stdz_alcohol')
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
spark_disconnect(sc)
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
#spark_disconnect(sc)
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
#spark_disconnect(sc)
wines_split <- wines |> sdf_random_split(training = 0.7, testing = 0.3, seed = 1337)
wines_train <- wines_split$training
wines_test <- wines_split$testing
wines_train |> sdf_describe(col=c('alcohol', 'volatile_acidity','sulphates'))
library(dbplot)
wines_train |> dbplot_raster(alcohol, fixed_acidity, fill=n(), resolution=50)
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_2$summary$r2adj
pred_1 <- ml_predict(fit_1, dataset = wines_test)
MSE_1 <- ml_regression_evaluator(
pred_1,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
pred_2 <- ml_predict(fit_2, dataset= wines_test)
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
MSE_1
MSE_2
library(dplyr)
wines_stats <- wines_train |>
summarize(
m = mean(alcohol),
s = sd(alcohol)
) |>
collect()
wines_train <- wines_train |>
mutate(stdz_alcohol = (alcohol - !!wines_stats$m)/ !!wines_stats$s)
spark_write_parquet(wines_train,path='wines/wines_train.parquet')
