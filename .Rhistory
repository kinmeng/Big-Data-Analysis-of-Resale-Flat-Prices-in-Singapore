library(corrr)
library(ggplot2)
df |> select(floor_area_sqm, remaining_lease_std, resale_price, dist_to_central,
dist_nearest_mall, dist_nearest_mrt) |>
correlate(use = "pairwise.complete.obs", method = "pearson") |>
shave(upper = TRUE) |>
rplot() + theme_dark() + theme(axis.text.x = element_text(angle = 45, vjust = 0.3, hjust=0.5)) + ggtitle("Correlation plot")
library(dbplot)
df |> dbplot_histogram(floor_area_sqm)
df |> dbplot_histogram(resale_price)
df |> dbplot_histogram(remaining_lease_std)
df |> mutate(log_price = log(resale_price)) |> collect() |> dbplot_histogram(log_price)
df |>
collect() |>
select(month, resale_price)|>
mutate(month = as.factor(month)) |>
ggplot(aes(month, resale_price)) +
geom_boxplot() + theme(axis.text.x = element_text(angle = 80, vjust = 1, hjust=1, size =5),
axis.ticks.margin=unit(0,'cm')) + xlab("Date") + ylab('Resale Flat Prices') +
ggtitle("Boxplots of Resale Flat Prices over time")
df |>
collect() |>
group_by(storey_range) |>
arrange(storey_range) |>
ggplot(aes(storey_range, resale_price)) +
geom_boxplot() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust=0.9), axis.ticks.margin=unit(0,'cm')) +
xlab("Storey Ranges") +
ylab("Resale Flat Prices") +
ggtitle("Boxplots of Resale Flat Prices over Storey Ranges")
scaler_model |>
ml_transform(df) |>
glimpse()
library(sparklyr)
library(dplyr)
# Connect to a local instance of Spark
sc <- spark_connect(master = "local", version = "3.3.0")
# Define the estimator
scaler <- ft_standard_scaler(
sc,
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
)
scaler
# Generate two vectors v and w, each with 100000 (pseudo) observations from
# a normal distribution, and use these to create the data frame rdf
rdf <- data.frame(
v = rnorm(100000, mean = 4, sd = 2),
w = rnorm(100000, mean = -4, sd = 0.5)
)
glimpse(rdf)
# Copy the rdf data frame to Spark and combine multiple vectors
df <- copy_to(sc, rdf)  |>
ft_vector_assembler(
input_cols = c("v", "w"),
output_col = "features"
)
glimpse(df)
disconnect(sc)
spark_disconnect(sc)
# Connect to a local instance of Spark
sc <- spark_connect(master = "local", version = "3.3.0")
# Define the estimator
scaler <- ft_standard_scaler(
sc,
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
)
# Define the estimator
scaler <- ft_standard_scaler(
sc,
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
)
scaler
# Generate two vectors v and w, each with 100000 (pseudo) observations from
# a normal distribution, and use these to create the data frame rdf
rdf <- data.frame(
v = rnorm(100000, mean = 4, sd = 2),
w = rnorm(100000, mean = -4, sd = 0.5)
)
glimpse(rdf)
# Copy the rdf data frame to Spark and combine multiple vectors
df <- copy_to(sc, rdf)  |>
ft_vector_assembler(
input_cols = c("v", "w"),
output_col = "features"
)
# Copy the rdf data frame to Spark and combine multiple vectors
df <- copy_to(sc, rdf)  |>
ft_vector_assembler(
input_cols = c("v", "w"),
output_col = "features"
)
# Fit the scaling model to the df dataset
scaler_model <- ml_fit(scaler, df)
# Fit the scaling model to the df dataset
scaler_model <- ml_fit(scaler, df)
scaler_model
scaler_model |>
ml_transform(df) |>
glimpse()
pipeline_model <- ml_fit(pipeline, df)
pipeline <- ml_pipeline(scaler)
pipeline_model <- ml_fit(pipeline, df)
pipeline_model
# Read a Parquet file from the LFS into a Spark DataFrame
okc_train <- spark_read_parquet(
sc,
path = "data/okc-train.parquet"
)
# Read a Parquet file from the LFS into a Spark DataFrame
okc_train <- spark_read_parquet(
sc,
path = "data/okc-train.parquet"
)
okc_train <- okc_train |>
select(not_working, age, essay_length)
pipeline <- ml_pipeline(sc) |>
ft_vector_assembler(
input_cols = c("age", "essay_length"),
output_col = "features"
) |>
ft_standard_scaler(
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
) |>
ml_logistic_regression(
features_col = "features_scaled",
label_col = "not_working"
)
pipeline <- ml_pipeline(sc) |>
ft_vector_assembler(
input_cols = c("age", "essay_length"),
output_col = "features"
) |>
ft_standard_scaler(
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
) |>
ml_logistic_regression(
features_col = "features_scaled",
label_col = "not_working"
)
#is_ml_estimator(pipeline) why is it an estimator not a transformer?
#This is because it hasn't been trained - now, you only have instructions for it, but
#it hasn't carried those instructions yet. That's why it's still an estimator
pipeline
okc_train |>
ft_vector_assembler(
input_cols = c("age", "essay_length"),
output_col = "features"
) |>
glimpse()
is_ml_estimator(pipeline)
okc_train |>
ft_vector_assembler(
input_cols = c("age", "essay_length"),
output_col = "features"
) |>
ft_standard_scaler(
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
) |>
glimpse()
pipeline_model <- ml_fit(pipeline, okc_train)
pipeline_model
# distinguish between model parameters and model hyperparameters (lambda is related to
# estimated method, thus, hyperparam - not sure abt this example, but shld be correct )
cv <- ml_cross_validator(
sc,
estimator = pipeline,
estimator_param_maps = list(
logistic_regression = list(
elastic_net_param = c(0.25, 0.75),
reg_param = c(0.001, 0.01)
)
),
evaluator = ml_binary_classification_evaluator(
sc,
label_col = "not_working"
),
num_folds = 10,
parallelism = 1,
seed = 1337
)
cv
cv_model <- ml_fit(cv, okc_train)
ml_validation_metrics(cv_model) |>
arrange(desc(areaUnderROC))
#a hybrid of ridge and lasso
cv_model$metric_name
cv_model$best_model
#read parquet file as df
df <- spark_read_parquet(sc, 'dataset/final.parquet')
#Ensure that the df has the same number of rows as the original dataset (a sanity check)
sdf_nrow(df)
df |> select(floor_area_sqm, remaining_lease_std, resale_price, dist_to_central,
dist_nearest_mall, dist_nearest_mrt) |>
correlate(use = "pairwise.complete.obs", method = "pearson") |>
shave(upper = TRUE) |>
rplot() + theme_dark() + theme(axis.text.x = element_text(angle = 45, vjust = 0.3, hjust=0.5)) + ggtitle("Correlation plot")
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
#spark_disconnect(sc)
wines_split <- wines |> sdf_random_split(training = 0.7, testing = 0.3, seed = 1337)
wines_train <- wines_split$training
wines_test <- wines_split$testing
wines_train |>
sdf_describe(col=c('alcohol','volatile_acidity','sulphates'))
wines_split <- wines |> sdf_random_split(training = 0.7, testing = 0.3, seed = 1337)
wines_train <- wines_split$training
wines_test <- wines_split$testing
wines_train
wines_train |>
sdf_describe(col=c('alcohol','volatile_acidity','sulphates'))
library(dbplot)
wines_train |> dbplot_raster(alcohol, fixed_acidity, fill=n(), resolution=50)
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol) |> tidy()
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates) |> tidy()
fit_2$summary$r2adj
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_2$summary$r2adj
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_2$summary$r2adj
pred_1 <- ml_predict(fit_1, dataset = wines_test)
MSE_1 <- ml_regression_evaluator(
pred_1,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
pred_2 <- ml_predict(fit_2, dataset= wines_test)
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
MSE_1
MSE_2
pred_2 <- ml_predict(fit_2, dataset= wines_test)
pred2
pred_2
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
MSE_1
MSE_2
wines_stats <- wines_train |>
summarize(
m = mean(alcohol),
s = sd(alcohol)
) |>
collect()
wines_train <- wines_train |>
mutate(stdz_alcohol = (alcohol - !!wines_stats$m)/ !!wines_stats$s)
sdf_describe(wines_train, c('stdz_alcohol'))
sdf_describe(wines_train,cols='stdz_alcohol')
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
spark_disconnect(sc)
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
#spark_disconnect(sc)
library(sparklyr)
sc <- spark_connect(master = "local", version="3.3.0")
wines <- spark_read_csv(
sc,
name = "red_wines",
path = "wines.csv",
escape = "\"",
options = list(multiline = TRUE),
memory = FALSE
)
#spark_disconnect(sc)
wines_split <- wines |> sdf_random_split(training = 0.7, testing = 0.3, seed = 1337)
wines_train <- wines_split$training
wines_test <- wines_split$testing
wines_train |> sdf_describe(col=c('alcohol', 'volatile_acidity','sulphates'))
library(dbplot)
wines_train |> dbplot_raster(alcohol, fixed_acidity, fill=n(), resolution=50)
fit_1 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol)
fit_2 <- wines_train |> ml_linear_regression(formula = quality ~ alcohol+volatile_acidity+sulphates)
fit_1$summary$r2adj
fit_2$summary$r2adj
pred_1 <- ml_predict(fit_1, dataset = wines_test)
MSE_1 <- ml_regression_evaluator(
pred_1,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
pred_2 <- ml_predict(fit_2, dataset= wines_test)
MSE_2 <- ml_regression_evaluator(
pred_2,
label_col = "quality",
prediction_col = "prediction",
metric_name = "mse"
)
MSE_1
MSE_2
library(dplyr)
wines_stats <- wines_train |>
summarize(
m = mean(alcohol),
s = sd(alcohol)
) |>
collect()
wines_train <- wines_train |>
mutate(stdz_alcohol = (alcohol - !!wines_stats$m)/ !!wines_stats$s)
spark_write_parquet(wines_train,path='wines/wines_train.parquet')
library(sparklyr)
library(dplyr)
library(lubridate)
sc <- spark_connect(master = "local", version = "3.3.0")
final_dataset <- spark_read_csv(sc, path = "final_dataset.csv")
final_dataset
#write parquet file
#spark_write_parquet(final_dataset,'dataset/final.parquet')
#read parquet file as df
df <- spark_read_parquet(sc, 'dataset/final.parquet')
df <- df |> collect() |> rename(date=month) |>
mutate(year = as.Date(as.character(date),'%Y')) |> mutate(year=year(year))
df <- df |> mutate(month = format(date,'%m')) |> mutate(month=as.numeric(month)) |> mutate(yearmonth=round(year+month/12,2))
df <- df |> select(-c('year','month'))
df <- copy_to(sc, df, overwrite = TRUE)
df
#Ensure that the df has the same number of rows as the original dataset (a sanity check)
sdf_nrow(df)
library(corrr)
library(ggplot2)
df |> select(floor_area_sqm, remaining_lease_std, resale_price, dist_to_central,
dist_nearest_mall, dist_nearest_mrt) |>
correlate(use = "pairwise.complete.obs", method = "pearson") |>
shave(upper = TRUE) |>
rplot() + theme_dark() + theme(axis.text.x = element_text(angle = 45, vjust = 0.3, hjust=0.5)) + ggtitle("Correlation plot")
library(dbplot)
df |> dbplot_histogram(floor_area_sqm)
df |> dbplot_histogram(resale_price)
df |> dbplot_histogram(remaining_lease_std)
df |> mutate(log_price = log(resale_price)) |> collect() |> dbplot_histogram(log_price)
df |>
dbplot_raster(x = floor_area_sqm, y = resale_price, fill = n(), resolution = 50)
df |>
dbplot_raster(x = dist_to_central, y = resale_price, fill = n(), resolution = 50)
df |>
dbplot_raster(x = dist_nearest_mall, y = resale_price, fill = n(), resolution = 50)
df |>
dbplot_raster(x = dist_nearest_mrt, y = resale_price, fill = n(), resolution = 50)
#Data Visualization
library(dplyr)
library(ggplot2)
library(tidyr)
df |>
collect() |>
select(date, resale_price)|>
mutate(date = as.factor(date)) |>
ggplot(aes(date, resale_price)) +
geom_boxplot() + theme(axis.text.x = element_text(angle = 80, vjust = 1, hjust=1, size =5),
axis.ticks.margin=unit(0,'cm')) + xlab("Date") + ylab('Resale Flat Prices') +
ggtitle("Boxplots of Resale Flat Prices over time")
#view quantiles computed for each record
#data <- df |> collect() |> group_by(month) |>summarise(quantile = list(round(quantile(resale_price,c(.10,.25,0.5,0.75)),1))) |> unnest_wider(quantile)
df |>
collect() |>
group_by(storey_range) |>
arrange(storey_range) |>
ggplot(aes(storey_range, resale_price)) +
geom_boxplot() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust=0.9), axis.ticks.margin=unit(0,'cm')) +
xlab("Storey Ranges") +
ylab("Resale Flat Prices") +
ggtitle("Boxplots of Resale Flat Prices over Storey Ranges")
library(sparklyr)
df
df |> ml_linear_regression(formula = resale_price ~ floor_area_sqm^2 + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
df |> log(resale_price) |> ml_linear_regression(formula = resale_price ~ floor_area_sqm^2 + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
df |> collect() |> log(resale_price) |> ml_linear_regression(formula = resale_price ~ floor_area_sqm^2 + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
df |> collect() |> mutate(resale_price=log(resale_price)) |> ml_linear_regression(formula = resale_price ~ floor_area_sqm^2 + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
df |> mutate(resale_price=log(resale_price)) |> ml_linear_regression(formula = resale_price ~ floor_area_sqm^2 + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
df |> mutate(resale_price=log(resale_price)) |> ml_linear_regression(formula = resale_price ~ floor_area_sqm + floor_area_sqm^2 + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
df |> ml_linear_regression(formula = resale_price ~ floor_area_sqm + floor_area_sqm^2 + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
df |> mutate(resale_price=log(resale_price)) |>ml_linear_regression(formula = resale_price ~ floor_area_sqm + floor_area_sqm^2 + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
df |> ml_linear_regression(formula = resale_price ~ floor_area_sqm + floor_area_sqm^2 + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
df |> ml_linear_regression(formula = resale_price ~ floor_area_sqm + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()
pipeline <- ml_pipeline(sc) |>
ft_string_indexer(
input_col = "flat_model", output_col = "flat_model_index"
) |>
ft_string_indexer(
input_col = "storey_range", output_col = "storey_range_index"
) |>
ft_vector_assembler(
input_cols = c("floor_area_sqm", "remaining_lease_std", "dist_to_central", "dist_nearest_mall", "dist_nearest_mrt", "flat_model_index", "storey_range_index", "yearmonth"),
output_col = "features"
) |>
ft_standard_scaler(
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
) |>
ml_linear_regression(
features_col = "features_scaled",
label_col = "resale_price"
)
pipeline
df_split <- df |>
sdf_random_split(training = 0.8, testing = 0.2, seed = 212)
df_train <- df_split$training
df_test <- df_split$testing
pipeline_model <- ml_fit(pipeline, df_train)
pipeline_model
cv <- ml_cross_validator(
sc,
estimator = pipeline,
estimator_param_maps = list(
linear_regression = list(
elastic_net_param = c(0, 1),
reg_param = seq(0, 1, 0.01)
)
),
evaluator = ml_regression_evaluator(
sc,
label_col = "resale_price"
),
num_folds = 10,
parallelism = 7,
seed = 212
)
cv
cv_model <- ml_fit(cv, df_train)
ml_validation_metrics(cv_model) |>
arrange(rmse)
#is_ml_transformer(cv_model)
cv_predict <- ml_predict(cv_model$best_model, df_test)
RMSE_1 <- ml_regression_evaluator(
cv_predict,
label_col = "resale_price",
prediction_col = "prediction",
metric_name = "rmse"
)
RMSE_1
ml_save(cv_model$best_model, path = "spark_model_hdb", overwrite = TRUE)
