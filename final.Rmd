---
title: "R Notebook"
output: html_notebook
---

```{r}

setwd(getwd())
library(readr)
combined<-read_csv('combined_data.csv') 
mallslatlon<-read_csv('malls_lat_lon.csv')
mrtlatlon<-read_csv('mrt_lat_lon.csv')

```

```{r}

library(dplyr)
mallslatlon <- mallslatlon |>           
  filter(Lon!='NULL', Lat!= "NULL", ) |>              #filtering all the non null data columns. 
  select(Malls,Lon,Lat)               #eliminating the first unwanted column from the dataset

```

```{r}


library(dplyr)
mrtlatlon <- mrtlatlon |>
  select(MRT_stations, Lon, Lat) |>
  filter(Lon!= "NULL", Lat!="NULL")



```

Selecting unique addresses only
```{r}

uniqueaddresses <- combined |>
  distinct(unique_add, .keep_all=TRUE)

```


Finding unique distances from MRT
```{r}

#install.packages("geosphere")
library('geosphere')


min_distance_mrt <- c()                  #create an empty vector to store future values in 

for (i in 1:nrow(uniqueaddresses)){
  house_lon<- as.numeric(uniqueaddresses[i,14])
  house_lat<-as.numeric(uniqueaddresses[i,15])
  mrt_vector<- (1:nrow(mrtlatlon)) #pre-allocate memory
  for(i in 1:nrow(mrtlatlon)){
    mrt_lon <- as.numeric(mrtlatlon[i,2])
    mrt_lat <- as.numeric(mrtlatlon[i,3])
    mrt_dist <- distm(c(house_lon,house_lat),c(mrt_lon,mrt_lat), fun = distCosine)/1000
    mrt_vector[i] <- mrt_dist
  }
  min_distance_mrt <- append(min_distance_mrt, min(mrt_vector))
}

min_distance_mrt




```

Finding unique Distances from Malls
```{r}



#install.packages("geosphere")
library('geosphere')


min_distance_mall <- c()                  #create an empty vector to store future values in 

for (i in 1:nrow(uniqueaddresses)){
  house_lon<- as.numeric(uniqueaddresses[i,14])
  house_lat<-as.numeric(uniqueaddresses[i,15])
  mall_vector<- (1:nrow(mallslatlon)) #pre-allocate memory
  for(i in 1:nrow(mallslatlon)){
    mall_lon <- as.numeric(mallslatlon[i,2])
    mall_lat <- as.numeric(mallslatlon[i,3])
    mall_dist <- distm(c(house_lon,house_lat),c(mall_lon,mall_lat), fun = distCosine)/1000
    mall_vector[i] <- mall_dist
  }
  min_distance_mall <- append(min_distance_mall, min(mall_vector))
}

min_distance_mall



```


Creating new Dataframe to store created values
```{r}

tojoin <- uniqueaddresses |>
  select(unique_add)

```


Adding Columns to Combined Dataframe

```{r}

tojoin[  , ncol(tojoin) + 1] <- min_distance_mall  

colnames(tojoin)[ncol(tojoin)] <- paste("dist_nearest_mall") 

tojoin[  , ncol(tojoin) + 1] <- min_distance_mrt 

colnames(tojoin)[ncol(tojoin)] <- paste("dist_nearest_mrt") 

tojoin



```

Using full join to attain final dataframe
```{r}

final <- left_join(combined, tojoin, by = "unique_add")

final

```

Removing Excess Data
```{r}

final <- final |>
  select(month, storey_range, floor_area_sqm, flat_model, remaining_lease_std, resale_price, unique_add, dist_to_central, dist_nearest_mall, dist_nearest_mrt, town)
  
write.csv(final, "final_dataset.csv", row.names = FALSE)  #creating a csv file with the above code
```


Adding COVID predictor 

```{r}

library(lubridate)

final <- read_csv("final_dataset.csv")

final <- final |>
  mutate(month = as.Date(paste(month, "-01", sep= ""))) |>
  mutate(Covid = case_when(month < as.Date("2020-02-01")~0, month >= as.Date("2020-02-01")~1))

write.csv(final, "final_dataset.csv", row.names = FALSE)
```



```{r}

library(sparklyr)
library(dplyr)
<<<<<<< HEAD

setwd("~/GitHub/DSA306-Big-Data-Analytics-Project")
=======
library(lubridate)

>>>>>>> 773368a8995309f409020de8dc7670af9d2e1267
sc <- spark_connect(master = "local", version = "3.3.0") 
final_dataset <- spark_read_csv(sc, path = "final_dataset.csv")
final_dataset

#write parquet file
#spark_write_parquet(final_dataset,'dataset/final.parquet')

#read parquet file as df
df <- spark_read_parquet(sc, 'dataset/final.parquet')


df <- df |> collect() |> rename(date=month) |>
  mutate(year = as.Date(as.character(date),'%Y')) |> mutate(year=year(year)) 
df <- df |> mutate(month = format(date,'%m')) |> mutate(month=as.numeric(month)) |> mutate(yearmonth=round(year+month/12,2))
df <- df |> select(-c('year','month'))
df <- copy_to(sc, df, overwrite = TRUE)
df
#Ensure that the df has the same number of rows as the original dataset (a sanity check)
sdf_nrow(df)

```

```{r}

<<<<<<< HEAD
library(corrr)
library(ggplot2)


df |> select(floor_area_sqm, remaining_lease_std, resale_price, dist_to_central,
         dist_nearest_mall, dist_nearest_mrt) |>
      correlate(use = "pairwise.complete.obs", method = "pearson") |>
      shave(upper = TRUE) |>
      rplot() + theme_dark() + theme(axis.text.x = element_text(angle = 45, vjust = 0.3, hjust=0.5)) + ggtitle("Correlation plot")



```

```{r}
#install.packages("ggrepel")
library(ggrepel)

indiv_town <- df |>
  select(town, resale_price, dist_to_central) |>
  group_by(town) |>
  summarise(cost_town = mean(resale_price), dist_town = mean(dist_to_central))


indiv_town_plot <- ggplot(data = indiv_town, aes(x = dist_town, y = cost_town)) + geom_label_repel(aes(label = town), size = 3)+ geom_point(aes(color = town), show.legend = FALSE) + geom_smooth(method = "lm", se = 0)

indiv_town_plot

```


```{r}

library(dbplot)
df |> dbplot_histogram(floor_area_sqm)
df |> dbplot_histogram(resale_price)
df |> dbplot_histogram(remaining_lease_std)
df |> mutate(log_price = log(resale_price)) |> collect() |> dbplot_histogram(log_price)

```


```{r}



=======
>>>>>>> 773368a8995309f409020de8dc7670af9d2e1267
df |>
dbplot_raster(x = floor_area_sqm, y = resale_price, fill = n(), resolution = 50)

df |>
dbplot_raster(x = dist_to_central, y = resale_price, fill = n(), resolution = 50) 

df |>
dbplot_raster(x = dist_nearest_mall, y = resale_price, fill = n(), resolution = 50)

df |>
dbplot_raster(x = dist_nearest_mrt, y = resale_price, fill = n(), resolution = 50)


```


```{r}
#Data Visualization
library(dplyr)
library(ggplot2)
library(tidyr) 


df |> 
  collect() |> 
  select(date, resale_price)|>
  mutate(date = as.factor(date)) |>
  ggplot(aes(date, resale_price)) +
  geom_boxplot() + theme(axis.text.x = element_text(angle = 80, vjust = 1, hjust=1, size =5), 
                         axis.ticks.margin=unit(0,'cm')) + xlab("Date") + ylab('Resale Flat Prices') +
  ggtitle("Boxplots of Resale Flat Prices over time")

#view quantiles computed for each record
#data <- df |> collect() |> group_by(month) |>summarise(quantile = list(round(quantile(resale_price,c(.10,.25,0.5,0.75)),1))) |> unnest_wider(quantile)


df |> 
  collect() |>
  group_by(storey_range) |> 
  arrange(storey_range) |>
  ggplot(aes(storey_range, resale_price)) +
  geom_boxplot() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust=0.9), axis.ticks.margin=unit(0,'cm')) +
  xlab("Storey Ranges") +
  ylab("Resale Flat Prices") +
  ggtitle("Boxplots of Resale Flat Prices over Storey Ranges")
  



```




```{r}
glimpse(df)
```

```{r}
<<<<<<< HEAD
df |> 
  ml_linear_regression(formula = resale_price ~ floor_area_sqm + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + Covid) |>
  summary()
```

```{r}
library(sparklyr)
library(dplyr)
=======
>>>>>>> 773368a8995309f409020de8dc7670af9d2e1267

library(sparklyr)
df 

df |> ml_linear_regression(formula = resale_price ~ floor_area_sqm + remaining_lease_std + dist_to_central + dist_nearest_mall + dist_nearest_mrt + storey_range + yearmonth) |>
summary()

```


Determine which variables to use as features.
unique_add replaced by the various distance measures.
month replaced by remaining_lease_std.

```{r}
pipeline <- ml_pipeline(sc) |>
  ft_string_indexer(
    input_col = "flat_model", output_col = "flat_model_index"
    ) |>
  ft_string_indexer(
    input_col = "storey_range", output_col = "storey_range_index"
    ) |>
  ft_vector_assembler(
<<<<<<< HEAD
    input_cols = c("floor_area_sqm", "remaining_lease_std", "dist_to_central", "dist_nearest_mall", "dist_nearest_mrt", "flat_model_index", "storey_range_index", "Covid"),
=======
    input_cols = c("floor_area_sqm", "remaining_lease_std", "dist_to_central", "dist_nearest_mall", "dist_nearest_mrt", "flat_model_index", "storey_range_index", "yearmonth"),
>>>>>>> 773368a8995309f409020de8dc7670af9d2e1267
    output_col = "features"
  ) |>
  ft_standard_scaler(
    input_col = "features",
    output_col = "features_scaled",
    with_mean = TRUE
  ) |>
  ml_linear_regression(
    features_col = "features_scaled",
    label_col = "resale_price"
  )

pipeline
```

Split df to train and test sets.

```{r}
df_split <- df |>
  sdf_random_split(training = 0.8, testing = 0.2, seed = 212)

df_train <- df_split$training
df_test <- df_split$testing
```



Fitting pipeline to model.

```{r}
pipeline_model <- ml_fit(pipeline, df_train)  
pipeline_model
```


Cross validate pipeline_model.

```{r}

cv <- ml_cross_validator(
  sc,
  estimator = pipeline,
  estimator_param_maps = list(
    linear_regression = list(
      elastic_net_param = c(0,1),
      reg_param = c(0.2,0.5,0.7)
    )
  ),
  evaluator = ml_regression_evaluator(
    sc,
    label_col = "resale_price"
  ),
  num_folds = 10,
  parallelism = 7,
  seed = 212
)

cv
```

```{r}
cv_model <- ml_fit(cv, df_train)

ml_validation_metrics(cv_model) |>
  arrange(rmse)
```


```{r}

#is_ml_transformer(cv_model)

cv_predict <- ml_predict(cv_model$best_model, df_test)


RMSE_1 <- ml_regression_evaluator(
cv_predict,
label_col = "resale_price",
prediction_col = "prediction",
metric_name = "rmse"
)

RMSE_1

```



Save ML model.

```{r}
ml_save(cv_model$best_model, path = "spark_model_hdb", overwrite = TRUE)
```

```{r}
library(sparklyr)
ml_load(sc, path="spark_model_hdb")


```


Plumber

```{r}
install.packages("plumber")
library(plumber)

plumb(file = "spark-plumber-hdb.R") |>
  pr_run(port = 8000)
```


